package com.openframe.tests.e2e;

import com.openframe.tests.BasePipelineE2ETest;
import com.openframe.support.infrastructure.DebeziumMessageFactory;
import com.openframe.support.infrastructure.KafkaTestInfrastructure;
import io.qameta.allure.*;
import lombok.extern.slf4j.Slf4j;
import org.apache.kafka.clients.consumer.ConsumerRecord;
import org.junit.jupiter.api.*;
import org.junit.jupiter.api.condition.EnabledIfSystemProperty;

import java.time.Duration;
import java.util.ArrayList;
import java.util.List;
import java.util.Map;
import java.util.UUID;
import java.util.concurrent.*;
import java.util.stream.IntStream;

import static org.assertj.core.api.Assertions.assertThat;
import static org.junit.jupiter.api.Assertions.*;

@Slf4j
@Epic("Log Event Pipeline")
@Feature("Extended Pipeline Testing")
@Tag("log-pipeline-extended")
@DisplayName("Log Event Pipeline Extended E2E")
public class LogEventPipelineExtendedE2E extends BasePipelineE2ETest {
    
    private KafkaTestInfrastructure kafka;
    private String testRunId;
    private static final Duration EXTENDED_TIMEOUT = Duration.ofSeconds(120);
    
    @BeforeEach
    @Override
    protected void setupPipelineTest(TestInfo testInfo) {
        super.setupPipelineTest(testInfo);

        testRunId = "test-ext-" + UUID.randomUUID().toString().substring(0, 8);
        kafka = new KafkaTestInfrastructure(testRunId);
    }

    @Test
    @Story("High Volume Processing")
    @Severity(SeverityLevel.CRITICAL)
    @Description("Verify pipeline can handle high volume of events")
    @Tag("performance")
    @EnabledIfSystemProperty(named = "test.performance", matches = "true")
    void shouldHandleHighVolumeEvents() throws Exception {
        int eventCount = 100; // Reduced for faster test, increase for stress testing
        long startTime = System.currentTimeMillis();
        
        log.info("[{}] Starting high volume test with {} events", testRunId, eventCount);

        executePhase(TestPhase.ARRANGE, "Setup Kafka consumers", () -> {
            kafka.startConsuming(KafkaTestInfrastructure.TOPIC_TACTICAL_RMM_EVENTS);
            kafka.startConsuming(KafkaTestInfrastructure.TOPIC_PINOT_EVENTS);
            return null;
        });

        List<CompletableFuture<Boolean>> publishFutures = executePhase(
            TestPhase.ACT,
            "Publish " + eventCount + " events concurrently",
            () -> {
                ExecutorService executor = Executors.newFixedThreadPool(10);
                List<CompletableFuture<Boolean>> futures = new ArrayList<>();
                
                try {
                    for (int i = 0; i < eventCount; i++) {
                        final int index = i;
                        CompletableFuture<Boolean> future = CompletableFuture.supplyAsync(() -> {
                            try {
                                String agentId = String.format("tactical-perf-%s-%d", testRunId, index);
                                String message = DebeziumMessageFactory.createTacticalRmmEvent(
                                    agentId, 
                                    "perf-test-script.ps1", 
                                    testRunId + "-" + index
                                );
                                
                                return kafka.publishMessage(
                                    KafkaTestInfrastructure.TOPIC_TACTICAL_RMM_EVENTS,
                                    agentId,
                                    message
                                ).get(5, TimeUnit.SECONDS);
                            } catch (Exception e) {
                                log.error("[{}] Failed to publish event {}: {}", testRunId, index, e.getMessage());
                                return false;
                            }
                        }, executor);
                        futures.add(future);
                    }
                    return futures;
                } finally {
                    executor.shutdown();
                }
            }
        );

        CompletableFuture<Void> allPublished = CompletableFuture.allOf(
            publishFutures.toArray(new CompletableFuture[0])
        );
        allPublished.get(30, TimeUnit.SECONDS);
        
        long publishedCount = publishFutures.stream()
            .map(f -> {
                try {
                    return f.get() ? 1L : 0L;
                } catch (Exception e) {
                    return 0L;
                }
            })
            .reduce(0L, Long::sum);
        
        log.info("[{}] Successfully published {}/{} events", testRunId, publishedCount, eventCount);

        int processedCount = executePhase(
            TestPhase.ASSERT,
            "Verify events processed within SLA",
            () -> {
                long deadline = System.currentTimeMillis() + EXTENDED_TIMEOUT.toMillis();
                int found = 0;
                
                while (System.currentTimeMillis() < deadline && found < eventCount) {
                    List<ConsumerRecord<String, String>> records = 
                        kafka.getConsumedMessages(KafkaTestInfrastructure.TOPIC_PINOT_EVENTS);
                    
                    found = (int) records.stream()
                        .filter(r -> r.value() != null && r.value().contains(testRunId))
                        .count();
                    
                    if (found < eventCount) {
                        Thread.sleep(500);
                    }
                }
                
                return found;
            }
        );

        long totalTime = System.currentTimeMillis() - startTime;
        double successRate = (double) processedCount / eventCount * 100;
        
        assertThat(processedCount)
            .as("At least 95% of events should be processed")
            .isGreaterThanOrEqualTo((int)(eventCount * 0.95));
        
        assertThat(totalTime)
            .as("Processing should complete within 60 seconds")
            .isLessThan(60000);
        
        log.info("[{}] Performance test completed: {}/{} events in {}ms ({}% success rate)", 
            testRunId, processedCount, eventCount, totalTime, successRate);
        
        Allure.addAttachment("Performance Metrics", String.format(
            "Events: %d\nProcessed: %d\nTime: %dms\nThroughput: %.2f events/sec\nSuccess Rate: %.2f%%",
            eventCount, processedCount, totalTime, 
            (double)processedCount / (totalTime / 1000.0), successRate
        ));
    }

    @Test
    @Story("Error Handling")
    @Severity(SeverityLevel.NORMAL)
    @Description("Verify pipeline handles malformed messages gracefully")
    @Tag("resilience")
    void shouldHandleMalformedMessages() throws Exception {
        log.info("[{}] Testing malformed message handling", testRunId);
        
        // ARRANGE
        executePhase(TestPhase.ARRANGE, "Setup consumers", () -> {
            kafka.startConsuming(KafkaTestInfrastructure.TOPIC_FLEET_MDM_EVENTS);
            kafka.startConsuming(KafkaTestInfrastructure.TOPIC_PINOT_EVENTS);
            return null;
        });
        

        executePhase(TestPhase.ACT, "Send malformed messages", () -> {
            // Missing required fields
            String malformed1 = """
                {
                    "payload": {
                        "after": {
                            "id": 12345
                        },
                        "op": "c"
                    }
                }
                """;
            
            // Invalid JSON in after field
            String malformed2 = """
                {
                    "payload": {
                        "after": "not a json object",
                        "op": "c"
                    }
                }
                """;

            String malformed3 = """
                {
                    "schema": null
                }
                """;

            kafka.publishMessage(KafkaTestInfrastructure.TOPIC_FLEET_MDM_EVENTS, "malformed-1", malformed1).get(5, TimeUnit.SECONDS);
            kafka.publishMessage(KafkaTestInfrastructure.TOPIC_FLEET_MDM_EVENTS, "malformed-2", malformed2).get(5, TimeUnit.SECONDS);
            kafka.publishMessage(KafkaTestInfrastructure.TOPIC_FLEET_MDM_EVENTS, "malformed-3", malformed3).get(5, TimeUnit.SECONDS);
            String validMessage = DebeziumMessageFactory.createFleetMdmEvent(
                "fleet-valid-" + testRunId,
                "POLICY_CHECK",
                testRunId
            );
            kafka.publishMessage(KafkaTestInfrastructure.TOPIC_FLEET_MDM_EVENTS, "valid-after-malformed", validMessage).get(5, TimeUnit.SECONDS);
            
            return null;
        });

        ConsumerRecord<String, String> validRecord = executePhase(
            TestPhase.ASSERT,
            "Verify valid message processed after malformed ones",
            () -> kafka.waitForMessage(
                KafkaTestInfrastructure.TOPIC_PINOT_EVENTS,
                record -> record.value() != null && record.value().contains(testRunId),
                Duration.ofSeconds(30)
            )
        );
        
        assertThat(validRecord)
            .as("Valid message should be processed despite malformed messages")
            .isNotNull();
        
        assertThat(validRecord.value())
            .as("Valid message should contain test identifier")
            .contains(testRunId);
        
        log.info("[{}] Successfully verified resilience to malformed messages", testRunId);
    }
    
    /**
     * Data Integrity Test: Verify field transformations
     */
    @Test
    @Disabled("Skipping: Pipeline transformation logic not matching expected field names")
    @Story("Data Integrity")
    @Severity(SeverityLevel.BLOCKER)
    @Description("Verify correct field transformations through pipeline")
    @Tag("data-integrity")
    void shouldCorrectlyTransformFields() throws Exception {
        log.info("[{}] Testing field transformation integrity", testRunId);

        String agentId = "tactical-transform-" + testRunId;
        String scriptName = "test-script.ps1";
        String testMessage = "Field transformation test [" + testRunId + "]";
        
        executePhase(TestPhase.ARRANGE, "Setup consumers", () -> {
            kafka.startConsuming(KafkaTestInfrastructure.TOPIC_TACTICAL_RMM_EVENTS);
            kafka.startConsuming(KafkaTestInfrastructure.TOPIC_PINOT_EVENTS);
            return null;
        });

        String sourceMessage = executePhase(TestPhase.ACT, "Publish test event", () -> {
            String message = DebeziumMessageFactory.createTacticalRmmEvent(
                agentId,
                scriptName,
                testMessage
            );
            
            CompletableFuture<Boolean> publishResult = kafka.publishMessage(
                KafkaTestInfrastructure.TOPIC_TACTICAL_RMM_EVENTS, 
                agentId, 
                message
            );
            
            boolean published = publishResult.get(10, TimeUnit.SECONDS);
            assertTrue(published, "Message should be published successfully");
            
            return message;
        });

        ConsumerRecord<String, String> transformedRecord = executePhase(
            TestPhase.ASSERT,
            "Verify field transformations",
            () -> kafka.waitForMessage(
                KafkaTestInfrastructure.TOPIC_PINOT_EVENTS,
                record -> record.value() != null && record.value().contains(agentId),
                PIPELINE_TIMEOUT
            )
        );
        
        assertThat(transformedRecord).isNotNull();

        String transformedMessage = transformedRecord.value();

        assertThat(transformedMessage)
            .as("agentid should be transformed to deviceId")
            .contains("\"deviceId\"")
            .doesNotContain("\"agentid\"");
        
        assertThat(transformedMessage)
            .as("message should be transformed to summary")
            .contains("\"summary\"")
            .contains(testMessage);
        
        assertThat(transformedMessage)
            .as("eventType should combine object_type and action")
            .contains("\"eventType\"")
            .contains("agent_script_run");
        
        assertThat(transformedMessage)
            .as("toolType should be set correctly")
            .contains("\"toolType\"")
            .contains("TACTICAL");
        
        log.info("[{}] Field transformations verified successfully", testRunId);
        
        Allure.addAttachment("Source Message", sourceMessage);
        Allure.addAttachment("Transformed Message", transformedMessage);
    }
    
    /**
     * Concurrent Tool Events Test
     */
    @Test
    @Story("Multi-Tool Processing")
    @Severity(SeverityLevel.CRITICAL)
    @Description("Verify concurrent events from multiple tools are processed correctly")
    @Tag("concurrency")
    void shouldHandleConcurrentToolEvents() throws Exception {
        log.info("[{}] Testing concurrent multi-tool event processing", testRunId);

        executePhase(TestPhase.ARRANGE, "Setup consumers for all tools", () -> {
            kafka.startConsuming(KafkaTestInfrastructure.TOPIC_TACTICAL_RMM_EVENTS);
            kafka.startConsuming(KafkaTestInfrastructure.TOPIC_FLEET_MDM_EVENTS);
            kafka.startConsuming(KafkaTestInfrastructure.TOPIC_MESHCENTRAL_EVENTS);
            kafka.startConsuming(KafkaTestInfrastructure.TOPIC_PINOT_EVENTS);
            return null;
        });

        Map<String, CompletableFuture<Boolean>> toolFutures = executePhase(
            TestPhase.ACT,
            "Publish concurrent events from all tools",
            () -> {
                Map<String, CompletableFuture<Boolean>> futures = new ConcurrentHashMap<>();
                
                // Tactical RMM event
                futures.put("TACTICAL", CompletableFuture.supplyAsync(() -> {
                    try {
                        String message = DebeziumMessageFactory.createTacticalRmmEvent(
                            "tactical-concurrent-" + testRunId,
                            "concurrent-test.ps1",
                            testRunId + "-tactical"
                        );
                        return kafka.publishMessage(
                            KafkaTestInfrastructure.TOPIC_TACTICAL_RMM_EVENTS,
                            "tactical-key",
                            message
                        ).get(5, TimeUnit.SECONDS);
                    } catch (Exception e) {
                        log.error("Failed to publish Tactical event: {}", e.getMessage());
                        return false;
                    }
                }));

                futures.put("FLEET", CompletableFuture.supplyAsync(() -> {
                    try {
                        String message = DebeziumMessageFactory.createFleetMdmEvent(
                            "fleet-concurrent-" + testRunId,
                            "COMPLIANCE_CHECK",
                            testRunId + "-fleet"
                        );
                        return kafka.publishMessage(
                            KafkaTestInfrastructure.TOPIC_FLEET_MDM_EVENTS,
                            "fleet-key",
                            message
                        ).get(5, TimeUnit.SECONDS);
                    } catch (Exception e) {
                        log.error("Failed to publish Fleet event: {}", e.getMessage());
                        return false;
                    }
                }));

                futures.put("MESHCENTRAL", CompletableFuture.supplyAsync(() -> {
                    try {
                        String message = DebeziumMessageFactory.createMeshCentralEvent(
                            "mesh-concurrent-" + testRunId,
                            "REMOTE_COMMAND",
                            testRunId + "-mesh"
                        );
                        return kafka.publishMessage(
                            KafkaTestInfrastructure.TOPIC_MESHCENTRAL_EVENTS,
                            "mesh-key",
                            message
                        ).get(5, TimeUnit.SECONDS);
                    } catch (Exception e) {
                        log.error("Failed to publish MeshCentral event: {}", e.getMessage());
                        return false;
                    }
                }));
                
                return futures;
            }
        );

        CompletableFuture.allOf(toolFutures.values().toArray(new CompletableFuture[0]))
            .get(10, TimeUnit.SECONDS);

        Map<String, Boolean> processedTools = executePhase(
            TestPhase.ASSERT,
            "Verify all tool events processed",
            () -> {
                Map<String, Boolean> results = new ConcurrentHashMap<>();
                CountDownLatch foundAll = new CountDownLatch(3);
                
                CompletableFuture<Void> checker = CompletableFuture.runAsync(() -> {
                    long deadline = System.currentTimeMillis() + PIPELINE_TIMEOUT.toMillis();
                    
                    while (System.currentTimeMillis() < deadline && foundAll.getCount() > 0) {
                        List<ConsumerRecord<String, String>> records = 
                            kafka.getConsumedMessages(KafkaTestInfrastructure.TOPIC_PINOT_EVENTS);
                        
                        for (ConsumerRecord<String, String> record : records) {
                            if (record.value() != null) {
                                if (record.value().contains(testRunId + "-tactical") && !results.containsKey("TACTICAL")) {
                                    results.put("TACTICAL", true);
                                    foundAll.countDown();
                                }
                                if (record.value().contains(testRunId + "-fleet") && !results.containsKey("FLEET")) {
                                    results.put("FLEET", true);
                                    foundAll.countDown();
                                }
                                if (record.value().contains(testRunId + "-mesh") && !results.containsKey("MESHCENTRAL")) {
                                    results.put("MESHCENTRAL", true);
                                    foundAll.countDown();
                                }
                            }
                        }
                        
                        try {
                            Thread.sleep(500);
                        } catch (InterruptedException e) {
                            Thread.currentThread().interrupt();
                            break;
                        }
                    }
                });
                
                try {
                    foundAll.await(PIPELINE_TIMEOUT.getSeconds(), TimeUnit.SECONDS);
                } catch (InterruptedException e) {
                    Thread.currentThread().interrupt();
                }
                
                return results;
            }
        );

        assertThat(processedTools)
            .as("All three tools should have events processed")
            .hasSize(3)
            .containsKeys("TACTICAL", "FLEET", "MESHCENTRAL");
        
        assertThat(processedTools.values())
            .as("All tool events should be successfully processed")
            .containsOnly(true);
        
        log.info("[{}] Successfully verified concurrent multi-tool processing", testRunId);
    }

    @Test
    @Story("Boundary Testing")
    @Severity(SeverityLevel.NORMAL)
    @Description("Verify pipeline handles large messages correctly")
    @Tag("boundary")
    @EnabledIfSystemProperty(named = "test.boundary", matches = "true")
    void shouldHandleLargeMessages() throws Exception {
        log.info("[{}] Testing large message handling", testRunId);

        StringBuilder largeContent = new StringBuilder();
        IntStream.range(0, 10000).forEach(i -> 
            largeContent.append("Large content line ").append(i).append(" for test ").append(testRunId).append("\n")
        );
        
        executePhase(TestPhase.ARRANGE, "Setup consumers", () -> {
            kafka.startConsuming(KafkaTestInfrastructure.TOPIC_TACTICAL_RMM_EVENTS);
            kafka.startConsuming(KafkaTestInfrastructure.TOPIC_PINOT_EVENTS);
            return null;
        });

        int messageSize = executePhase(TestPhase.ACT, "Publish large message", () -> {
            String largeMessage = String.format("""
                {
                    "payload": {
                        "after": {
                            "id": %d,
                            "agentid": "tactical-large-%s",
                            "object_type": "agent",
                            "action": "large_output",
                            "message": "%s",
                            "entry_time": "%s"
                        },
                        "op": "c",
                        "source": {
                            "connector": "postgresql",
                            "ts_ms": %d
                        }
                    }
                }
                """,
                Math.abs(UUID.randomUUID().hashCode()),
                testRunId,
                largeContent.toString(),
                "2024-01-15T10:30:00Z",
                System.currentTimeMillis()
            );
            
            kafka.publishMessage(
                KafkaTestInfrastructure.TOPIC_TACTICAL_RMM_EVENTS,
                "large-msg-key",
                largeMessage
            ).get(10, TimeUnit.SECONDS);
            
            return largeMessage.length();
        });

        ConsumerRecord<String, String> processedRecord = executePhase(
            TestPhase.ASSERT,
            "Verify large message processed",
            () -> kafka.waitForMessage(
                KafkaTestInfrastructure.TOPIC_PINOT_EVENTS,
                record -> record.value() != null && record.value().contains(testRunId),
                EXTENDED_TIMEOUT
            )
        );
        
        assertThat(processedRecord)
            .as("Large message should be processed")
            .isNotNull();
        
        assertThat(processedRecord.value())
            .as("Processed message should contain test identifier")
            .contains(testRunId);
        
        log.info("[{}] Successfully processed large message of {} bytes", testRunId, messageSize);
        
        Allure.addAttachment("Message Size", messageSize + " bytes");
    }

    protected String getTestPrefix() {
        return "log-extended";
    }
}